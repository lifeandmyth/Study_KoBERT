{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KoBERT finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "id": "-sx87sgK7_pz",
    "outputId": "9f1c67bf-7c67-45d7-88b7-4bbc7384a29b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://****@github.com/lifeandmyth/study_KoBERT.git@master\n",
      "  Cloning https://****@github.com/lifeandmyth/study_KoBERT.git (to revision master) to c:\\users\\hi\\appdata\\local\\temp\\pip-req-build-vnlqddck\n",
      "  Resolved https://****@github.com/lifeandmyth/study_KoBERT.git to commit 5a513f92e565f4d032177261440e6d9fcdddeb98\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting boto3>=1.14.0 (from kobert==0.2.3)\n",
      "  Obtaining dependency information for boto3>=1.14.0 from https://files.pythonhosted.org/packages/7e/47/7613bc648576e5be0da6c7feb2a5cfba1f6b36e7b163adbdc5ce4ebd8f16/boto3-1.28.52-py3-none-any.whl.metadata\n",
      "  Downloading boto3-1.28.52-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting gluonnlp>=0.6.0 (from kobert==0.2.3)\n",
      "  Using cached gluonnlp-0.10.0.tar.gz (344 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting mxnet>=1.4.0 (from kobert==0.2.3)\n",
      "  Using cached mxnet-1.7.0.post2-py2.py3-none-win_amd64.whl (33.1 MB)\n",
      "Requirement already satisfied: onnxruntime<=1.16.0,==1.16.0 in c:\\users\\hi\\anaconda3\\envs\\kobert_c_venv\\lib\\site-packages (from kobert==0.2.3) (1.16.0)\n",
      "Collecting sentencepiece>=0.1.6 (from kobert==0.2.3)\n",
      "  Downloading sentencepiece-0.1.99-cp311-cp311-win_amd64.whl (977 kB)\n",
      "     ---------------------------------------- 0.0/977.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 10.2/977.5 kB ? eta -:--:--\n",
      "     ---- ----------------------------------- 112.6/977.5 kB 1.7 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 522.2/977.5 kB 4.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  972.8/977.5 kB 6.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 977.5/977.5 kB 5.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: torch>=1.7.0 in c:\\users\\hi\\anaconda3\\envs\\kobert_c_venv\\lib\\site-packages (from kobert==0.2.3) (2.0.1+cu117)\n",
      "Collecting transformers>=4.8.1 (from kobert==0.2.3)\n",
      "  Obtaining dependency information for transformers>=4.8.1 from https://files.pythonhosted.org/packages/1a/06/3817f9bb923437ead9a794f0ac0d03b8b5e0478ab112db4c413dd37c09da/transformers-4.33.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.33.2-py3-none-any.whl.metadata (119 kB)\n",
      "     ---------------------------------------- 0.0/119.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 119.9/119.9 kB 6.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\hi\\anaconda3\\envs\\kobert_c_venv\\lib\\site-packages (from onnxruntime<=1.16.0,==1.16.0->kobert==0.2.3) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\hi\\anaconda3\\envs\\kobert_c_venv\\lib\\site-packages (from onnxruntime<=1.16.0,==1.16.0->kobert==0.2.3) (23.5.26)\n",
      "Requirement already satisfied: numpy>=1.24.2 in c:\\users\\hi\\anaconda3\\envs\\kobert_c_venv\\lib\\site-packages (from onnxruntime<=1.16.0,==1.16.0->kobert==0.2.3) (1.26.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hi\\anaconda3\\envs\\kobert_c_venv\\lib\\site-packages (from onnxruntime<=1.16.0,==1.16.0->kobert==0.2.3) (23.1)\n",
      "Requirement already satisfied: protobuf in c:\\users\\hi\\anaconda3\\envs\\kobert_c_venv\\lib\\site-packages (from onnxruntime<=1.16.0,==1.16.0->kobert==0.2.3) (4.24.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\hi\\anaconda3\\envs\\kobert_c_venv\\lib\\site-packages (from onnxruntime<=1.16.0,==1.16.0->kobert==0.2.3) (1.12)\n",
      "Collecting botocore<1.32.0,>=1.31.52 (from boto3>=1.14.0->kobert==0.2.3)\n",
      "  Obtaining dependency information for botocore<1.32.0,>=1.31.52 from https://files.pythonhosted.org/packages/fa/a4/96371c65dfed960a86ed2f90e6b5ae023167c66c249969c86d05ca0aa2d7/botocore-1.31.52-py3-none-any.whl.metadata\n",
      "  Downloading botocore-1.31.52-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.14.0->kobert==0.2.3)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3>=1.14.0->kobert==0.2.3)\n",
      "  Obtaining dependency information for s3transfer<0.7.0,>=0.6.0 from https://files.pythonhosted.org/packages/d9/17/a3b666f5ef9543cfd3c661d39d1e193abb9649d0cfbbfee3cf3b51d5af02/s3transfer-0.6.2-py3-none-any.whl.metadata\n",
      "  Downloading s3transfer-0.6.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting cython (from gluonnlp>=0.6.0->kobert==0.2.3)\n",
      "  Obtaining dependency information for cython from https://files.pythonhosted.org/packages/ea/8f/216de5d7bede3e26a7131b427a8aadade032f03f9c8ee88792def02e2cf4/Cython-3.0.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached Cython-3.0.2-cp311-cp311-win_amd64.whl.metadata (3.2 kB)\n",
      "INFO: pip is looking at multiple versions of mxnet to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting mxnet>=1.4.0 (from kobert==0.2.3)\n",
      "  Using cached mxnet-1.7.0.post1-py2.py3-none-win_amd64.whl (33.0 MB)\n",
      "  Using cached mxnet-1.6.0-py2.py3-none-win_amd64.whl (26.9 MB)\n",
      "  Using cached mxnet-1.5.0-py2.py3-none-win_amd64.whl (23.5 MB)\n",
      "  Using cached mxnet-1.4.1-py2.py3-none-win_amd64.whl (21.9 MB)\n",
      "  Using cached mxnet-1.4.0.post0-py2.py3-none-win_amd64.whl (21.9 MB)\n",
      "  Using cached mxnet-1.4.0-py2.py3-none-win_amd64.whl (21.9 MB)\n",
      "Collecting gluonnlp>=0.6.0 (from kobert==0.2.3)\n",
      "  Using cached gluonnlp-0.9.2.tar.gz (252 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "INFO: pip is still looking at multiple versions of mxnet to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached gluonnlp-0.9.1.tar.gz (252 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached gluonnlp-0.9.0.post0.tar.gz (252 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached gluonnlp-0.8.3.tar.gz (236 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached gluonnlp-0.8.2.tar.gz (237 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached gluonnlp-0.8.1.tar.gz (236 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached gluonnlp-0.8.0.tar.gz (235 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached gluonnlp-0.7.1.tar.gz (233 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached gluonnlp-0.7.0.tar.gz (233 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached gluonnlp-0.6.0.tar.gz (209 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting onnxruntime<=1.16.0,==1.16.0 (from kobert==0.2.3)\n",
      "  Obtaining dependency information for onnxruntime<=1.16.0,==1.16.0 from https://files.pythonhosted.org/packages/0c/14/9966e648a48f9bfee456002700cde4ca41f1e04ebaea842ccf663b6e7db2/onnxruntime-1.16.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached onnxruntime-1.16.0-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
      "\n",
      "The conflict is caused by:\n",
      "    onnxruntime 1.16.0 depends on numpy>=1.24.2\n",
      "    gluonnlp 0.6.0 depends on numpy\n",
      "    mxnet 1.7.0.post2 depends on numpy<1.17.0 and >=1.8.2\n",
      "    onnxruntime 1.16.0 depends on numpy>=1.24.2\n",
      "    gluonnlp 0.6.0 depends on numpy\n",
      "    mxnet 1.7.0.post1 depends on numpy<1.17.0 and >=1.8.2\n",
      "    onnxruntime 1.16.0 depends on numpy>=1.24.2\n",
      "    gluonnlp 0.6.0 depends on numpy\n",
      "    mxnet 1.6.0 depends on numpy<1.17.0 and >=1.8.2\n",
      "    onnxruntime 1.16.0 depends on numpy>=1.24.2\n",
      "    gluonnlp 0.6.0 depends on numpy\n",
      "    mxnet 1.5.0 depends on numpy<1.17.0 and >=1.8.2\n",
      "    onnxruntime 1.16.0 depends on numpy>=1.24.2\n",
      "    gluonnlp 0.6.0 depends on numpy\n",
      "    mxnet 1.4.1 depends on numpy<1.17.0 and >=1.8.2\n",
      "    onnxruntime 1.16.0 depends on numpy>=1.24.2\n",
      "    gluonnlp 0.6.0 depends on numpy\n",
      "    mxnet 1.4.0.post0 depends on numpy<1.15.0 and >=1.8.2\n",
      "    onnxruntime 1.16.0 depends on numpy>=1.24.2\n",
      "    gluonnlp 0.6.0 depends on numpy\n",
      "    mxnet 1.4.0 depends on numpy<1.15.0 and >=1.8.2\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet 'https://****@github.com/lifeandmyth/study_KoBERT.git' 'C:\\Users\\hi\\AppData\\Local\\Temp\\pip-req-build-vnlqddck'\n",
      "ERROR: Cannot install kobert because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n"
     ]
    }
   ],
   "source": [
    "# !pip install ipywidgets  # for vscode\n",
    "## !pip install git+https://git@github.com/SKTBrain/KoBERT.git@master => onnxruntime 1.8.0이 배포 버전에 없어서 안됨\n",
    "!pip install git+https://git@github.com/lifeandmyth/study_KoBERT.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5mTNl7BKT2Fx"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mxnet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnotebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmxnet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgluon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmxnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gluon\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmxnet\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmx\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mxnet'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from mxnet.gluon import nn\n",
    "from mxnet import gluon\n",
    "import mxnet as mx\n",
    "import gluonnlp as nlp\n",
    "\n",
    "from kobert import get_mxnet_kobert_model\n",
    "from kobert import get_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cc-zco-ST2F_"
   },
   "source": [
    "### Loading KoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU\n",
    "ctx = mx.cpu()\n",
    "\n",
    "# GPU\n",
    "# ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "wI841Zb38XOn",
    "outputId": "f9794e99-c913-4ca0-b8fd-6e15ce9d74c7"
   },
   "outputs": [],
   "source": [
    "bert_base, vocab = get_mxnet_kobert_model(use_decoder=False, use_classifier=False, ctx=ctx, cachedir=\".cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "NijpWe8J8isZ",
    "outputId": "d03d1cc1-327f-4b44-ed66-f02126687688"
   },
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "colab_type": "code",
    "id": "i69AUj9gT2Gk",
    "outputId": "050d42de-ac07-4c04-9f14-b5ea411df008"
   },
   "outputs": [],
   "source": [
    "ds = gluon.data.SimpleDataset([['나 보기가 역겨워', '김소월']])\n",
    "trans = nlp.data.BERTSentenceTransform(tok, max_seq_length=10)\n",
    "\n",
    "list(ds.transform(trans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "colab_type": "code",
    "id": "4qy9g_UMVtdj",
    "outputId": "c4546df4-ce5b-4484-e245-309c90fed014"
   },
   "outputs": [],
   "source": [
    "!wget -O .cache/ratings_train.txt http://skt-lsl-nlp-model.s3.amazonaws.com/KoBERT/datasets/nsmc/ratings_train.txt\n",
    "!wget -O .cache/ratings_test.txt http://skt-lsl-nlp-model.s3.amazonaws.com/KoBERT/datasets/nsmc/ratings_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4LfCTweqT2Gt"
   },
   "outputs": [],
   "source": [
    "dataset_train = nlp.data.TSVDataset(\".cache/ratings_train.txt\", field_indices=[1,2], num_discard_samples=1)\n",
    "dataset_test = nlp.data.TSVDataset(\".cache/ratings_test.txt\", field_indices=[1,2], num_discard_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pt0raV8uT2G2"
   },
   "outputs": [],
   "source": [
    "class BERTDataset(mx.gluon.data.Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "        sent_dataset = gluon.data.SimpleDataset([[\n",
    "            i[sent_idx],\n",
    "        ] for i in dataset])\n",
    "        self.sentences = sent_dataset.transform(transform)\n",
    "        self.labels = gluon.data.SimpleDataset(\n",
    "            [np.array(np.int32(i[label_idx])) for i in dataset])\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vtk-8pQST2G9"
   },
   "outputs": [],
   "source": [
    "max_len = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_K_BLZP_T2HF"
   },
   "outputs": [],
   "source": [
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rhaw0H4ST2HM"
   },
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Block):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 num_classes=2,\n",
    "                 dropout=None,\n",
    "                 prefix=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__(prefix=prefix, params=params)\n",
    "        self.bert = bert\n",
    "        with self.name_scope():\n",
    "            self.classifier = nn.HybridSequential(prefix=prefix)\n",
    "            if dropout:\n",
    "                self.classifier.add(nn.Dropout(rate=dropout))\n",
    "            self.classifier.add(nn.Dense(units=num_classes))\n",
    "\n",
    "    def forward(self, inputs, token_types, valid_length=None):\n",
    "        _, pooler = self.bert(inputs, token_types, valid_length)\n",
    "        return self.classifier(pooler)\n",
    "                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y00BOPwST2HX"
   },
   "outputs": [],
   "source": [
    "model = BERTClassifier(bert_base, num_classes=2, dropout=0.1)\n",
    "# 분류 레이어만 초기화 한다. \n",
    "model.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "model.hybridize()\n",
    "\n",
    "# softmax cross entropy loss for classification\n",
    "loss_function = gluon.loss.SoftmaxCELoss()\n",
    "\n",
    "metric = mx.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A2dLhnHkT2Hf"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "lr = 5e-5\n",
    "\n",
    "train_dataloader = mx.gluon.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = mx.gluon.data.DataLoader(data_test, batch_size=int(batch_size/2), num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ESo76UH-T2Hr"
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(model.collect_params(), 'bertadam',\n",
    "                        {'learning_rate': lr, 'epsilon': 1e-9, 'wd':0.01})\n",
    "\n",
    "log_interval = 4\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wspMBDOAT2H0"
   },
   "outputs": [],
   "source": [
    "# LayerNorm과 Bias에는 Weight Decay를 적용하지 않는다. \n",
    "for _, v in model.collect_params('.*beta|.*gamma|.*bias').items():\n",
    "    v.wd_mult = 0.0\n",
    "params = [\n",
    "    p for p in model.collect_params().values() if p.grad_req != 'null'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NCR6AMKHT2H6"
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, data_iter, ctx=ctx):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    i = 0\n",
    "    for i, (t,v,s, label) in enumerate(data_iter):\n",
    "        token_ids = t.as_in_context(ctx)\n",
    "        valid_length = v.as_in_context(ctx)\n",
    "        segment_ids = s.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = model(token_ids, segment_ids, valid_length.astype('float32'))\n",
    "        acc.update(preds=output, labels=label)\n",
    "        if i > 1000:\n",
    "            break\n",
    "        i += 1\n",
    "    return(acc.get()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SkcW6GyeT2IA"
   },
   "outputs": [],
   "source": [
    "#learning rate warmup을 위한 준비 \n",
    "accumulate = 4\n",
    "step_size = batch_size * accumulate if accumulate else batch_size\n",
    "num_train_examples = len(data_train)\n",
    "num_train_steps = int(num_train_examples / step_size * num_epochs)\n",
    "warmup_ratio = 0.1\n",
    "num_warmup_steps = int(num_train_steps * warmup_ratio)\n",
    "step_num = 0\n",
    "all_model_params = model.collect_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yf_rpZTq6uES"
   },
   "outputs": [],
   "source": [
    "# Set grad_req if gradient accumulation is required\n",
    "if accumulate and accumulate > 1:\n",
    "    for p in params:\n",
    "        p.grad_req = 'add'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 984
    },
    "colab_type": "code",
    "id": "0mJ3Pw_VT2IH",
    "outputId": "abc9ecfb-8674-445f-cd5d-3fcd57252f39"
   },
   "outputs": [],
   "source": [
    "for epoch_id in range(num_epochs):\n",
    "    metric.reset()\n",
    "    step_loss = 0\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "        if step_num < num_warmup_steps:\n",
    "            new_lr = lr * step_num / num_warmup_steps\n",
    "        else:\n",
    "            non_warmup_steps = step_num - num_warmup_steps\n",
    "            offset = non_warmup_steps / (num_train_steps - num_warmup_steps)\n",
    "            new_lr = lr - offset * lr\n",
    "        trainer.set_learning_rate(new_lr)\n",
    "        with mx.autograd.record():\n",
    "            # load data to GPU\n",
    "            token_ids = token_ids.as_in_context(ctx)\n",
    "            valid_length = valid_length.as_in_context(ctx)\n",
    "            segment_ids = segment_ids.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "\n",
    "            # forward computation\n",
    "            out = model(token_ids, segment_ids, valid_length.astype('float32'))\n",
    "            ls = loss_function(out, label).mean()\n",
    "\n",
    "        # backward computation\n",
    "        ls.backward()\n",
    "        if not accumulate or (batch_id + 1) % accumulate == 0:\n",
    "          trainer.allreduce_grads()\n",
    "          nlp.utils.clip_grad_global_norm(params, 1)\n",
    "          trainer.update(accumulate if accumulate else 1)\n",
    "          step_num += 1\n",
    "          if accumulate and accumulate > 1:\n",
    "              # set grad to zero for gradient accumulation\n",
    "              all_model_params.zero_grad()\n",
    "\n",
    "        step_loss += ls.asscalar()\n",
    "        metric.update([label], [out])\n",
    "        if (batch_id + 1) % (50) == 0:\n",
    "            print('[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.10f}, acc={:.3f}'\n",
    "                         .format(epoch_id + 1, batch_id + 1, len(train_dataloader),\n",
    "                                 step_loss / log_interval,\n",
    "                                 trainer.learning_rate, metric.get()[1]))\n",
    "            step_loss = 0\n",
    "    test_acc = evaluate_accuracy(model, test_dataloader, ctx)\n",
    "    print('Test Acc : {}'.format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "naver_review_classifications_gluon_bert.ipynb의 사본",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "kobert_c_venv",
   "language": "python",
   "name": "kobert_c_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
